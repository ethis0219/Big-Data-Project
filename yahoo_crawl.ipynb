{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "class Requester(object):\n",
    "    headers = {\n",
    "        'Accept':'*/*' ,\n",
    "        'Accept-Encoding':'gzip, deflate, sdch, br' ,\n",
    "        'Accept-Language':'zh-TW,zh;q=0.8,ja;q=0.6,en-US;q=0.4,en;q=0.2,zh-CN;q=0.2' ,\n",
    "        'Connection':'keep-alive',    \n",
    "        'Host':'tw.bid.yahoo.com' ,     \n",
    "        \n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.84 Safari/537.36',\n",
    "        'X-PJAX':'true',\n",
    "        'X-Requested-With':'XMLHttpRequest' \n",
    "    }\n",
    "    global rs\n",
    "    def req(self,url):\n",
    "        connection = True\n",
    "        soup=''\n",
    "        rs = requests.session()\n",
    "        while connection == True:           \n",
    "            try:                \n",
    "                res = rs.get(url,headers=self.headers)                \n",
    "                soup = BeautifulSoup(res.text)                \n",
    "                connection = False\n",
    "                \n",
    "                return soup\n",
    "            except:\n",
    "                print 'connection error , sleep 1 min'                          \n",
    "                time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Requester import Requester\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "  \n",
    "class CategoryCrawler(Requester):\n",
    "    next =''\n",
    "    domainUrl = 'https://tw.bid.yahoo.com/tw/'\n",
    "    leafUrl=[]\n",
    "    result=''\n",
    "    resultCount = 0                         \n",
    "   \n",
    "    def __init__(self , url) :          #設定CategoryCrawler建構子\n",
    "        self.url = url\n",
    "        self.hasNext = True                      \n",
    "    def firstCraw(self):                #爬第一次，目的是取回下一頁連結\n",
    "        soup = super(CategoryCrawler,self).req(self.url)\n",
    "        try:\n",
    "            nextpage = soup.select('.next-page a')[0]['href']\n",
    "        except:\n",
    "            pass\n",
    "        self.targetTag=(soup.select('#srp_bc span')[-1].text).encode('utf-8')   #這裡python內(unicode環境)->編碼為utf-8(str)\n",
    "        self.categoryTag=(soup.select('#srp_bc span')[-2].text).encode('utf-8')    \n",
    "        print self.targetTag,self.categoryTag                   #抓一下這個最小類別的名稱作檔案名，類別名稱作為資料夾\n",
    "        instantBuy=soup.select('.yui3-u.hasbuyp a')[0]['href']                              \n",
    "        return instantBuy\n",
    "        \n",
    "    def urlGather(self,url):                                #把一頁的網址收集起來 回傳一個List\n",
    "        soup = super(CategoryCrawler,self).req(url)\n",
    "        urlList =list()\n",
    "        for title in soup.select('.srp-pdtitle'):\n",
    "           \n",
    "            #print title.text\n",
    "            urlList.append(title.select('a')[0]['href'])\n",
    "            \n",
    "            \n",
    "        try:   \n",
    "            self.next =soup.select('.next-page.yui3-u a')[0]['href']               #如果現在爬取的這頁是最後一頁\n",
    "        except:\n",
    "            self.hasNext = False             \n",
    "        return urlList     \n",
    "    def contentCraw(self,url):\n",
    "        soup = super(CategoryCrawler,self).req(url)\n",
    "        content =''\n",
    "        title = soup.select('.title')[0].text\n",
    "        price = soup.select('.number')[0].text\n",
    "        salCount = self.numberCleaner(soup.select('.has-sold')[0].text)\n",
    "        remark = self.numberCleaner(soup.select('.remark')[0].text)                \n",
    "        sallerName = soup.select('.seller-name a')[0].text   \n",
    "        question = self.numberCleaner(soup.select('.total-item-count ')[0].text)         \n",
    "        qicUrl = soup.select('.main-image')[0]['src']\n",
    "        for font in soup.select('font'):         \n",
    "            if font.text !='':\n",
    "                tempC=re.sub('[\\s+]', '', font.text)\n",
    "                tempC = tempC.strip(' \\t\\n\\r')             \n",
    "                content += tempC\n",
    "        result =title+'_|'+url+'_|'+salCount+'_|'+price+'_|'+sallerName+'_|'+remark+'_|'+question+'_|'+qicUrl+'_|'+content+'$& \\n'\n",
    "                #標題, 商品網址, 已賣數量, 價格, 賣家名稱 , 賣家評價 , 問與答數量 , 圖片網址 , 內文\n",
    "        return result.encode('utf-8')\n",
    "             \n",
    "    def getThisCate(self,actualPage,index):\n",
    "        #組合網址，先拆出pg \n",
    "        address=actualPage.split('&')\n",
    "        #塞入第n頁\n",
    "        pg = re.sub(r'pg=[\\d]+', 'pg={}'.format(index),actualPage)\n",
    "        aoffest = re.sub(r'aoffset=[\\d]+', 'aoffset={}'.format((index-1)*60),pg)       \n",
    "        #address[-6]= 'pg={}'.format(index)\n",
    "        #address[-8]= 'aoffset={}'.format((index-1)*60)\n",
    "        #放回actualPage\n",
    "        actualPage = '&'.join(address)\n",
    "        if index==1 :\n",
    "            aoffest = self.domainUrl+aoffest+'&pjax=1' \n",
    "        \n",
    "        return aoffest \n",
    "    def numberCleaner(self,st):\n",
    "        match = re.search(r'[\\d,]+',st)\n",
    "        num = match.group().encode('utf-8')\n",
    "        return  num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tagcrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from Requester import Requester\n",
    "import codecs\n",
    "class TagCrawler(Requester) :\n",
    "    leafName = []\n",
    "\n",
    "    def __init__(self , url) :          #設定TagCrawler建構子\n",
    "        self.url = url                  #啟動url\n",
    "                        \n",
    "    def allsubCraw(self):\n",
    "        soup = super(TagCrawler,self).req(self.url)\n",
    "        root ={}\n",
    "        for url in soup.select('.more a'):  \n",
    "            root[url.text] = url['href']\n",
    "        return  root \n",
    "    def leafCraw(self,bigUrl):\n",
    "        soup = super(TagCrawler,self).req(bigUrl)\n",
    "        leafUrl = []\n",
    "        \n",
    "        rg = 'leaf'\n",
    "        for url in soup.select('.title'):            \n",
    "            isLeaf=re.search(rg,url['href'])\n",
    "            try:\n",
    "                isLeaf.group()              #y拍的小項網址裡面會有leaf這個字，利用這點濾掉不是小項的網址\n",
    "                leafUrl.append(url['href'])                \n",
    "                self.leafName.append(url.text)\n",
    "            except:\n",
    "                   continue\n",
    "        return leafUrl\n",
    "    def startCraw(self):\n",
    "        tc = TagCrawler('https://tw.bid.yahoo.com/tw/0-all.html?.r=1465414707')    \n",
    "        root = tc.allsubCraw()\n",
    "        leafUrl=[]    \n",
    "        for url in root:\n",
    "            leafUrl.extend(tc.leafCraw(root[url]))\n",
    "        print len(leafUrl)\n",
    "    # if main 裡面是單獨測試碼，直接執行可以得到測試結果                          \n",
    "# if __name__ == '__main__':\n",
    "#     tc = TagCrawler('https://tw.bid.yahoo.com/tw/0-all.html?.r=1465414707')    \n",
    "#     root = tc.allsubCraw()\n",
    "#     leafUrl=[]    \n",
    "#     for url in root:\n",
    "#         leafUrl.extend(tc.leafCraw(root[url]))\n",
    "#     with codecs.open('name.txt' ,'w','utf-8')as t:\n",
    "#         t.write('\\n'.join(tc.leafName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maincrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女裝與服飾配件所有類別 complete\n",
      "運動、戶外與休閒所有類別 complete\n",
      "圖書/影音/文具所有類別 complete\n",
      "男性精品與服飾所有類別 complete\n",
      "電腦、平板與周邊所有類別 complete\n",
      "汽機車精品百貨所有類別 complete\n",
      "美容保養與彩妝所有類別 complete\n",
      "家電與影音視聽所有類別 complete\n"
     ]
    }
   ],
   "source": [
    "from TagCrawler import * \n",
    "from  CategoryCrawler import *\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tc = TagCrawler('https://tw.bid.yahoo.com/tw/0-all.html?.r=1465414707')\n",
    "    rootDir = tc.allsubCraw()\n",
    "    leafUrl=[]\n",
    "    result=''\n",
    "    resultCount = 0                         \n",
    "    id = 3210                               #改變爬取品項的起點\n",
    "    endId = 3249                              #爬取品項停止點     \n",
    "    for url in rootDir:\n",
    "        leafUrl.extend(tc.leafCraw(rootDir[url]))\n",
    "        print url ,'complete'\n",
    "        time.sleep(1)\n",
    "    print len(leafUrl)\n",
    "    urlList = []\n",
    "    \n",
    "    for url in leafUrl[id:endId]:\n",
    "        \n",
    "        index =1\n",
    "        urlList = [] \n",
    "        cc = CategoryCrawler(url)\n",
    "        actualPage = cc.firstCraw()        \n",
    "            \n",
    "        while cc.hasNext:       \n",
    "            # pg改成 .format形式在塞回成網址             \n",
    "            actualPage=cc.getThisCate(actualPage,index)\n",
    "             \n",
    "            urlList.extend(cc.urlGather(actualPage))\n",
    "             \n",
    "            time.sleep(1)\n",
    "            #如果沒有下一頁 停止程式 po 出搜集到的url個數\n",
    "            if cc.hasNext ==False:\n",
    "                index =1            \n",
    "                print '沒有下一頁 {}完成'.format(cc.targetTag)\n",
    "                print len(urlList)  \n",
    "            else:\n",
    "                    index+=1\n",
    "                    \n",
    "        addr = 'E:/yahoo_result'+'/'+str(id)+'.txt'\n",
    "        addr.encode('utf-8')\n",
    "        f = open(addr ,'w')\n",
    "        id += 1\n",
    "                \n",
    "        for i in urlList :            \n",
    "            try:\n",
    "                \n",
    "                result += cc.contentCraw(i)\n",
    "                resultCount += 1 \n",
    "                if resultCount ==10:                    #改變 resultCount可以加快寫進速度，注意被ban\n",
    "                    f.write(result+'\\n')\n",
    "                    resultCount =0\n",
    "                    result = ''            \n",
    "                \n",
    "            except:\n",
    "                print '發生錯誤'\n",
    "                addre = 'E:/yahoo_result/error.txt'\n",
    "                error = open(addre ,'a')\n",
    "                error.write(i+'\\n')\n",
    "                continue\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
