{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tagcrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from Requester import Requester\n",
    "class TagCrawler(Requester) :\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.\\\n",
    "                94 Safari/537.36'\n",
    "               }\n",
    "    def __init__(self , url) :          #設定TagCrawler建構子\n",
    "        self.url = url                  #啟動url\n",
    "                        \n",
    "    def allsubCraw(self):\n",
    "        soup = super(TagCrawler,self).req(self.url)\n",
    "        root ={}\n",
    "        for url in soup.select('.more a'):  \n",
    "            root[url.text] = url['href']\n",
    "        return  root \n",
    "    def leafCraw(self,bigUrl):\n",
    "        soup = super(TagCrawler,self).req(bigUrl)\n",
    "        leafUrl = []\n",
    "        rg = 'leaf'\n",
    "        for url in soup.select('.title'):            \n",
    "            isLeaf=re.search(rg,url['href'])\n",
    "            try:\n",
    "                isLeaf.group()              #y拍的小項網址裡面會有leaf這個字，利用這點濾掉不是小項的網址\n",
    "                leafUrl.append(url['href'])\n",
    "            except:\n",
    "                   continue\n",
    "        return leafUrl\n",
    "    def startCraw(self):\n",
    "        tc = TagCrawler('https://tw.bid.yahoo.com/tw/0-all.html?.r=1465414707')    \n",
    "        root = tc.allsubCraw()\n",
    "        leafUrl=[]    \n",
    "        for url in root:\n",
    "            leafUrl.extend(tc.leafCraw(root[url]))\n",
    "        print len(leafUrl)\n",
    "    # if main 裡面是單獨測試碼，直接執行可以得到測試結果                          \n",
    "# if __name__ == '__main__':\n",
    "#     tc = TagCrawler('https://tw.bid.yahoo.com/tw/0-all.html?.r=1465414707')    \n",
    "#     root = tc.allsubCraw()\n",
    "#     leafUrl=[]    \n",
    "#     for url in root:\n",
    "#         leafUrl.extend(tc.leafCraw(root[url]))\n",
    "#     print len(leafUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "class Requester(object):\n",
    "    headers = {\n",
    "        'Accept':'*/*' ,\n",
    "        'Accept-Encoding':'gzip, deflate, sdch, br' ,\n",
    "        'Accept-Language':'zh-TW,zh;q=0.8,ja;q=0.6,en-US;q=0.4,en;q=0.2,zh-CN;q=0.2' ,\n",
    "        'Connection':'keep-alive',    \n",
    "        'Host':'tw.bid.yahoo.com' ,\n",
    "        \n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.84 Safari/537.36',\n",
    "        'X-PJAX':'true',\n",
    "        'X-Requested-With':'XMLHttpRequest' \n",
    "    }\n",
    "    def req(self,url):\n",
    "        connection = True\n",
    "        soup=''\n",
    "        rs = requests.session()\n",
    "        while connection == True:           \n",
    "            try:                \n",
    "                res = rs.get(url,headers=self.headers)                \n",
    "                soup = BeautifulSoup(res.text)                \n",
    "                connection = False\n",
    "                \n",
    "                return soup\n",
    "            except:\n",
    "                print 'connection error , sleep 1 min'                          \n",
    "                time.sleep(60)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Requester import Requester\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "class CategoryCrawler(Requester):\n",
    "    next =''\n",
    "    domainUrl = 'https://tw.bid.yahoo.com/tw/'\n",
    "    def __init__(self , url) :          #設定CategoryCrawler建構子\n",
    "        self.url = url\n",
    "        self.hasNext = True                      \n",
    "    def firstCraw(self):                #爬第一次，目的是取回下一頁連結\n",
    "        soup = super(CategoryCrawler,self).req(self.url)\n",
    "        nextpage = soup.select('.next-page a')[0]['href']\n",
    "        self.targetTag=(soup.select('#srp_bc span')[-1].text).encode('utf-8')    #這裡python內(unicode環境)->編碼為utf-8(str)\n",
    "        print self.targetTag  \n",
    "        instantBuy=soup.select('.yui3-u.hasbuyp a')[0]['href']                              #抓一下這個最小類別的名稱\n",
    "        return instantBuy\n",
    "        \n",
    "    def urlGather(self,url):                                #把一頁的網址收集起來 回傳一個List\n",
    "        soup = super(CategoryCrawler,self).req(self.url)\n",
    "        urlList =list()\n",
    "        for title in soup.select('.srp-pdtitle'):\n",
    "            print title.text\n",
    "            #print title.text\n",
    "            urlList.append(title.select('a')[0]['href'])\n",
    "            \n",
    "        try:   \n",
    "            self.next =soup.select('.next-page.yui3-u a')[0]['href']               #如果現在爬取的這頁是最後一頁\n",
    "        except:\n",
    "            self.hasNext = False             \n",
    "        return urlList     \n",
    "    def contentCraw(self,url):\n",
    "        soup = super(CategoryCrawler,self).req(self.url)\n",
    "        content =''\n",
    "        title = soup.select('.title')[0].text\n",
    "        price = soup.select('.number')[0].text\n",
    "        salCount = self.numberCleaner(soup.select('.has-sold')[0].text)\n",
    "        remark = self.numberCleaner(soup.select('.remark')[0].text)                \n",
    "        sallerName = soup.select('.seller-name a')[0].text   \n",
    "        question = self.numberCleaner(soup.select('.total-item-count ')[0].text)         \n",
    "        qicUrl = soup.select('.main-image')[0]['src']\n",
    "        result =title+'_|'+url+'_|'+salCount+'_|'+price+'_|'+sallerName+'_|'+remark+'_|'+question+'_|'+qicUrl+'_|'+content\n",
    "                #標題, 商品網址, 已賣數量, 價格, 賣家名稱 , 賣家評價 , 問與答數量 , 圖片網址 , 內文\n",
    "        return result\n",
    "             \n",
    "    def getThisCate(self,actualPage,index):\n",
    "        #組合網址，先拆出pg \n",
    "        address=actualPage.split('&')\n",
    "        #塞入第n頁        \n",
    "        address[-5]= 'pg={}'.format(index)\n",
    "        address[-7]= 'aoffset={}'.format((index-1)*60)\n",
    "        #放回actualPage\n",
    "        actualPage = '&'.join(address)\n",
    "        \n",
    "        return actualPage \n",
    "    def numberCleaner(self,st):\n",
    "        match = re.search(r'[\\d,]+',st)\n",
    "        num = match.group().encode('utf-8')\n",
    "        return  num            \n",
    "    # if main 裡面是單獨測試碼，直接執行可以得到測試結果         \n",
    "# if __name__ == '__main__':\n",
    "#     #在這裡需要輸入從標籤進來以後的第一個頁面   \n",
    "#     cc = CategoryCrawler('https://tw.bid.yahoo.com/tw/2092109949-category-leaf.html?.r=1465376779?hpp=23336_cat_category')\n",
    "#     #爬第一次  \n",
    "#                                                                                   # ^^^^^^^^^^^^ 似乎只需要改這裡就能換標籤\n",
    "#     actualPage = cc.firstCraw()     \n",
    "#     urlList = []\n",
    "#     index =1\n",
    "#     #如果物件的hasNext=True 則執行迴圈\n",
    "     \n",
    "#     while cc.hasNext:       \n",
    "#     # pg改成 .format形式在塞回成網址        \n",
    "#         #actualPage=cc.getThisCate(actualPage,index)\n",
    "        \n",
    "        \n",
    "#         print actualPage    \n",
    "#         urlList.extend(cc.urlGather(actualPage))\n",
    "#         actualPage = cc.domainUrl+cc.next\n",
    "     \n",
    "          \n",
    "#         time.sleep(3)\n",
    "#      #如果沒有下一頁 停止程式 po 出搜集到的url個數\n",
    "#         if cc.hasNext ==False:\n",
    "#             index =1            \n",
    "#             print '沒有下一頁 {}完成'.format(cc.targetTag)\n",
    "#             print len(urlList)  \n",
    "#         else:\n",
    "#                 index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maincrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from TagCrawler import * \n",
    "from  CategoryCrawler import *\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tc = TagCrawler('https://tw.bid.yahoo.com/tw/0-all.html?.r=1465414707')\n",
    "    rootDir = tc.allsubCraw()\n",
    "    leafUrl=[]    \n",
    "    for url in rootDir:\n",
    "        leafUrl.extend(tc.leafCraw(rootDir[url]))\n",
    "        print url ,'完成'\n",
    "        time.sleep(1)\n",
    "    print len(leafUrl)\n",
    "    urlList = []\n",
    "    \n",
    "    for url in leafUrl[25:26]:\n",
    "        index =1\n",
    "        urlList = [] \n",
    "        cc = CategoryCrawler(url)\n",
    "        actualPage = cc.firstCraw()        \n",
    "            \n",
    "        while cc.hasNext:       \n",
    "            # pg改成 .format形式在塞回成網址             \n",
    "            actualPage=cc.getThisCate(actualPage,index)\n",
    "            page = cc.domainUrl+actualPage\n",
    "            urlList.extend(cc.urlGather(page))\n",
    "            print page\n",
    "            time.sleep(1)\n",
    "             #如果沒有下一頁 停止程式 po 出搜集到的url個數\n",
    "            if cc.hasNext ==False:\n",
    "                index =1            \n",
    "                print '沒有下一頁 {}完成'.format(cc.targetTag)\n",
    "                print len(urlList)  \n",
    "            else:\n",
    "                    index+=1\n",
    "        f = codecs.open('e:\\result\\{}.txt'.format(cc.targetTag),'w')        \n",
    "        for i in urlList :\n",
    "            result=cc.contentCraw(i)\n",
    "            f.write(result+'\\n')        \n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
