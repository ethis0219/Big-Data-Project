{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pip install scikit-learn\n",
    "- pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "[u'and', u'document', u'first', u'is', u'one', u'second', u'the', u'third', u'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This is the second second document.',\n",
    "     'And the third one.',\n",
    "     'Is this the first document?',\n",
    "    ]\n",
    "x = vectorizer.fit_transform(corpus)  #將文本中的詞語轉換為詞頻矩陣\n",
    "print x.toarray()  #看到詞頻矩陣的結果，矩陣元素a[i][j] 表示j詞在第i個文本下的詞頻\n",
    "\n",
    "print vectorizer.get_feature_names()  #看到所有文本的關鍵字\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------這裡輸出第 1 篇文本的詞語tf-idf權重------\n",
      "and   0.0\n",
      "document   0.438776742859\n",
      "first   0.541976569726\n",
      "is   0.438776742859\n",
      "one   0.0\n",
      "second   0.0\n",
      "the   0.358728738248\n",
      "third   0.0\n",
      "this   0.438776742859\n",
      "-------這裡輸出第 2 篇文本的詞語tf-idf權重------\n",
      "and   0.0\n",
      "document   0.272301467523\n",
      "first   0.0\n",
      "is   0.272301467523\n",
      "one   0.0\n",
      "second   0.853225736145\n",
      "the   0.222624292325\n",
      "third   0.0\n",
      "this   0.272301467523\n",
      "-------這裡輸出第 3 篇文本的詞語tf-idf權重------\n",
      "and   0.552805319991\n",
      "document   0.0\n",
      "first   0.0\n",
      "is   0.0\n",
      "one   0.552805319991\n",
      "second   0.0\n",
      "the   0.28847674875\n",
      "third   0.552805319991\n",
      "this   0.0\n",
      "-------這裡輸出第 4 篇文本的詞語tf-idf權重------\n",
      "and   0.0\n",
      "document   0.438776742859\n",
      "first   0.541976569726\n",
      "is   0.438776742859\n",
      "one   0.0\n",
      "second   0.0\n",
      "the   0.358728738248\n",
      "third   0.0\n",
      "this   0.438776742859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()  #TfidfTransformer是統計vectorizer中每個詞語的tf-idf權值\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This is the second second document.',\n",
    "     'And the third one.',\n",
    "     'Is this the first document?',\n",
    "    ]\n",
    "\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "a = x.toarray()\n",
    "\n",
    "tfidf = transformer.fit_transform(a)\n",
    "weight = tfidf.toarray()\n",
    "word = vectorizer.get_feature_names()\n",
    "#print word\n",
    "#print weight\n",
    "for i in range(len(weight)):\n",
    "    print u\"-------這裡輸出第\",i+1,u\"篇文本的詞語tf-idf權重------\"\n",
    "    for j in range(len(word)):\n",
    "        print word[j] + '   ' + str(weight[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算每篇文本的高頻字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\BIG DATA\\Anaconda2\\Lib\\site-packages\\jieba\\dict.txt ...\n",
      "Dumping model to file cache c:\\users\\bigdat~1\\appdata\\local\\temp\\jieba.ubead6589120cfc220ac511b567ef3b82.cache\n",
      "Loading model cost 1.282 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba, operator, os, json\n",
    "jieba.set_dictionary('C:\\Users\\BIG DATA\\Anaconda2\\Lib\\site-packages\\jieba\\dict.txt.big')\n",
    "#jieba.load_userdict(\"C:\\Users\\BIG DATA\\Anaconda2\\Lib\\site-packages\\jieba\\userdict.txt\")\n",
    "\n",
    "path = 'C:\\Users\\BIG DATA\\ptt_gift'  #改成自己存txt檔的路徑\n",
    "path1 = 'C:\\Users\\BIG DATA\\ptt_gift_jieba'  #新建一個資料夾, 放計算高頻的檔案\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    with open(os.path.join(path, filename), 'r') as f:\n",
    "        for ele in f.readlines():\n",
    "            \n",
    "            not_in_dic_ingre = {}\n",
    "            words = list(jieba.cut(ele, cut_all=False))\n",
    "           \n",
    "            text = ''\n",
    "            for word in words:\n",
    "                if word in not_in_dic_ingre:\n",
    "                    not_in_dic_ingre[word] = not_in_dic_ingre[word] + 1\n",
    "                else:\n",
    "                    not_in_dic_ingre[word] = 1\n",
    "                text = text + word\n",
    "            \n",
    "            sorted_x = sorted(not_in_dic_ingre.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "            with open(path1+\"/\"+filename.replace(\".txt\",\"-freq.txt\"), 'w') as g:\n",
    "                for item in sorted_x:\n",
    "                    p = (item[0] + ',' + str(item[1]) + '\\n')\n",
    "        #             if len(item[0]) >= 1:\n",
    "                    g.write(p.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 幫每篇文本分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba, operator, os, json\n",
    "jieba.set_dictionary(\"C:\\Users\\BIG DATA\\Anaconda2\\Lib\\site-packages\\jieba\\dict.txt.big\")\n",
    "# jieba.load_userdict(\"C:\\Users\\BIG DATA\\Anaconda2\\Lib\\site-packages\\jieba\\userdict.txt\")\n",
    "\n",
    "path = 'C:\\Users\\BIG DATA\\ptt_gift'\n",
    "path1 = 'C:\\Users\\BIG DATA\\ptt_gift_jieba'    #新建一個資料夾, 放分詞後的檔案\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "#     print \"Loading: %s\" % filename\n",
    "    with open(os.path.join(path, filename), 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            words = jieba.cut(line, cut_all=False)\n",
    "\n",
    "            result = ''\n",
    "            for seg in words:\n",
    "                result = result + seg + ' '\n",
    "\n",
    "            with open(path1+\"/\"+filename.replace(\".txt\",\"-seg.txt\"),\"w\") as g:\n",
    "                g.write(result.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 刪除停止詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 先將停止詞轉換格式成 [,$,0,1,2,3,4,5,6,7,8,9,?,_,“,”,、,。]\n",
    "with open('C:\\\\Users\\\\BIG DATA\\\\stopkey.txt','r') as f:\n",
    "    with open('C:\\\\Users\\\\BIG DATA\\\\stopkey_trans.txt','w') as g:\n",
    "        for ele in f.readlines():\n",
    "            eles = ele.strip('\\n')\n",
    "            g.write(eles+',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "stopkey=[]\n",
    "for line in open('C:\\Users\\BIG DATA\\stopkey_trans.txt','r'):\n",
    "    for ele in line.split(','):\n",
    "        stopkey.append(ele)\n",
    "\n",
    "path = 'C:\\Users\\BIG DATA\\ptt_gift_jieba'\n",
    "path1 = 'C:\\Users\\BIG DATA\\ptt_gift_clean'\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "#     print \"Loading: %s\" % filename\n",
    "    with open(os.path.join(path, filename), 'r') as f:\n",
    "        for ele in f.readlines():\n",
    "            eles = ele.split(' ')\n",
    "            \n",
    "            texts=''\n",
    "            for word in eles:\n",
    "                if word not in stopkey:\n",
    "                    texts = texts + word + ' '\n",
    "                    \n",
    "            with open(path1+\"/\"+filename.replace(\"-seg.txt\",\"-clean.txt\"),'w') as g:\n",
    "                g.write(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算每篇文本的tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, string\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()  #TfidfTransformer是統計vectorizer中每個詞語的tf-idf權值\n",
    "\n",
    "path = 'C:\\Users\\BIG DATA\\ptt_gift_word'\n",
    "path1 = 'C:\\Users\\BIG DATA\\ptt_gift_tfidf'\n",
    "file_data = []\n",
    "dic={}\n",
    "for filename in os.listdir(path):\n",
    "#     print \"Loading: %s\" % filename\n",
    "    with open(os.path.join(path, filename), 'r') as f:\n",
    "        file_data.append(f.read())\n",
    "\n",
    "        tfidf = transformer.fit_transform(vectorizer.fit_transform(file_data))\n",
    "        word = vectorizer.get_feature_names() #所有文本的關鍵字\n",
    "        weight = tfidf.toarray()              #對應的tfidf矩陣\n",
    "\n",
    "#         result = ''\n",
    "#         for seg in word:\n",
    "#             result = result + seg + ' '\n",
    "\n",
    "with open(path1+\"/\"+\"tfidf3.txt\",\"w\") as g:\n",
    "    for i in range(len(weight)):\n",
    "        g.write(\"-------這裡輸出第\"+str(i+1)+\"篇文本的詞語tf-idf權重------\"+\"\\n\")\n",
    "        for j in range(len(word)):\n",
    "            g.write(word[j].encode('utf-8')+\"    \"+str(weight[i][j])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: u'\\u4e00', 2: u'\\u4e8c', 3: u'\\u4e09'}\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "b=[u'一',u'二',u'三']\n",
    "c={}\n",
    "for i in xrange(0,3):\n",
    "    c[a[i]]=b[i]\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
